"use strict";(globalThis.webpackChunkt_1_c_docs=globalThis.webpackChunkt_1_c_docs||[]).push([[571],{4369:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"spikeyolo-v8-tracker/dataset-format","title":"Dataset Format","description":"eTraM Dataset Structure","source":"@site/docs/spikeyolo-v8-tracker/dataset-format.md","sourceDirName":"spikeyolo-v8-tracker","slug":"/spikeyolo-v8-tracker/dataset-format","permalink":"/docs/spikeyolo-v8-tracker/dataset-format","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Installation & Setup","permalink":"/docs/spikeyolo-v8-tracker/installation"},"next":{"title":"Configuration Guide","permalink":"/docs/spikeyolo-v8-tracker/configuration"}}');var t=s(4848),a=s(8453);const r={sidebar_position:4},l="Dataset Format",o={},c=[{value:"eTraM Dataset Structure",id:"etram-dataset-structure",level:2},{value:"Data Splits",id:"data-splits",level:2},{value:"Event Data Format (.h5 files)",id:"event-data-format-h5-files",level:2},{value:"HDF5 Structure",id:"hdf5-structure",level:3},{value:"Event Data Characteristics",id:"event-data-characteristics",level:3},{value:"Example: Loading Event Data",id:"example-loading-event-data",level:3},{value:"Annotation Format (.npy files)",id:"annotation-format-npy-files",level:2},{value:"Structured Array Fields",id:"structured-array-fields",level:3},{value:"Bounding Box Format",id:"bounding-box-format",level:3},{value:"Example: Loading Annotations",id:"example-loading-annotations",level:3},{value:"Dynamic Class Configuration",id:"dynamic-class-configuration",level:2},{value:"Example Configuration",id:"example-configuration",level:3},{value:"Previous Class Mappings (for reference)",id:"previous-class-mappings-for-reference",level:3},{value:"Annotation Characteristics",id:"annotation-characteristics",level:2},{value:"Sample Sequence Analysis",id:"sample-sequence-analysis",level:2},{value:"Data Loading Implementation",id:"data-loading-implementation",level:2},{value:"Key Components",id:"key-components",level:3},{value:"Dynamic Batching Process",id:"dynamic-batching-process",level:3},{value:"1. Dynamic Sample Calculation",id:"1-dynamic-sample-calculation",level:4},{value:"2. Overlapping Window Sampling",id:"2-overlapping-window-sampling",level:4},{value:"3. Temporal Annotation Matching",id:"3-temporal-annotation-matching",level:4},{value:"Memory Requirements",id:"memory-requirements",level:2},{value:"Camera Specifications",id:"camera-specifications",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"dataset-format",children:"Dataset Format"})}),"\n",(0,t.jsx)(e.h2,{id:"etram-dataset-structure",children:"eTraM Dataset Structure"}),"\n",(0,t.jsx)(e.p,{children:"The project is designed to work with the eTraM (Event-based Traffic Monitoring) dataset format."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"eTraM/\n\u251c\u2500\u2500 HDF5/\n\u2502   \u251c\u2500\u2500 train_h5_6/\n\u2502   \u2502   \u251c\u2500\u2500 train_night_0040_td.h5      # Event data\n\u2502   \u2502   \u251c\u2500\u2500 train_night_0040_bbox.npy   # Grouped annotations (3 classes)\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 val_h5_1/\n\u2502   \u2514\u2500\u2500 test_h5_1/\n\u2514\u2500\u2500 class annotations/\n    \u251c\u2500\u2500 eight_class_annotations_train/\n    \u2502   \u251c\u2500\u2500 train_night_0040_bbox.npy   # Fine-grained annotations (8 classes)\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 eight_class_annotations_val/\n    \u2514\u2500\u2500 eight_class_annotations_test/\n"})}),"\n",(0,t.jsx)(e.h2,{id:"data-splits",children:"Data Splits"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Train"}),": ~112 sequences (mix of day/night)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Val"}),": ~23 sequences (mix of day/night)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Test"}),": ~30 sequences (mix of day/night)"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"event-data-format-h5-files",children:"Event Data Format (.h5 files)"}),"\n",(0,t.jsx)(e.h3,{id:"hdf5-structure",children:"HDF5 Structure"}),"\n",(0,t.jsx)(e.p,{children:"Event data is stored in HDF5 format with the following structure:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"events/\n\u251c\u2500\u2500 x: uint16 array        # X coordinates (0-1279)\n\u251c\u2500\u2500 y: uint16 array        # Y coordinates (0-719)\n\u251c\u2500\u2500 p: int16 array         # Polarity (0=negative, 1=positive)\n\u251c\u2500\u2500 t: int64 array         # Timestamps in microseconds\n\u251c\u2500\u2500 width: int64 scalar    # Image width (1280)\n\u2514\u2500\u2500 height: int64 scalar   # Image height (720)\n"})}),"\n",(0,t.jsx)(e.h3,{id:"event-data-characteristics",children:"Event Data Characteristics"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Resolution"}),": 1280\xd7720 pixels"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Event Count"}),": ~17M events per sequence (5-6 seconds)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Resolution"}),": Microsecond precision"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Polarity Distribution"}),": ~50/50 positive/negative events"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Duration"}),": 5-6 seconds per sequence"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"example-loading-event-data",children:"Example: Loading Event Data"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import h5py\nimport numpy as np\n\nwith h5py.File('train_night_0040_td.h5', 'r') as f:\n    events = {\n        'x': f['events/x'][:],\n        'y': f['events/y'][:],\n        'p': f['events/p'][:],\n        't': f['events/t'][:],\n        'width': f['events/width'][()],\n        'height': f['events/height'][()]\n    }\n"})}),"\n",(0,t.jsx)(e.h2,{id:"annotation-format-npy-files",children:"Annotation Format (.npy files)"}),"\n",(0,t.jsx)(e.h3,{id:"structured-array-fields",children:"Structured Array Fields"}),"\n",(0,t.jsx)(e.p,{children:"Annotations are stored as NumPy structured arrays with the following dtype:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"dtype = [\n    ('t', '<i8'),           # Timestamp (int64)\n    ('x', '<f4'),           # Top-left X coordinate (float32)\n    ('y', '<f4'),           # Top-left Y coordinate (float32)\n    ('w', '<f4'),           # Bounding box width (float32)\n    ('h', '<f4'),           # Bounding box height (float32)\n    ('class_id', '<u4'),    # Class identifier (uint32)\n    ('track_id', '<u4'),    # Object tracking ID (uint32)\n    ('class_confidence', '<f4')  # Detection confidence (float32)\n]\n"})}),"\n",(0,t.jsx)(e.h3,{id:"bounding-box-format",children:"Bounding Box Format"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Format"}),": ",(0,t.jsx)(e.code,{children:"(x, y, w, h)"})," - top-left corner + width/height"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Coordinates"}),": Pixel coordinates in the image space"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Alignment"}),": Annotations synchronized with event timestamps"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"example-loading-annotations",children:"Example: Loading Annotations"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import numpy as np\n\nannotations = np.load('train_night_0040_bbox.npy')\n\n# Access fields\ntimestamps = annotations['t']\nbboxes = np.column_stack([\n    annotations['x'],\n    annotations['y'],\n    annotations['w'],\n    annotations['h']\n])\nclass_ids = annotations['class_id']\ntrack_ids = annotations['track_id']\n"})}),"\n",(0,t.jsx)(e.h2,{id:"dynamic-class-configuration",children:"Dynamic Class Configuration"}),"\n",(0,t.jsxs)(e.p,{children:["The project supports ",(0,t.jsx)(e.strong,{children:"dynamic class configuration"})," through ",(0,t.jsx)(e.code,{children:"config.yaml"}),". Classes are defined in the ",(0,t.jsx)(e.code,{children:"classes"})," list, and the number of classes is automatically detected."]}),"\n",(0,t.jsx)(e.h3,{id:"example-configuration",children:"Example Configuration"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:"classes:\n  - Pedestrian\n  - Car\n  - Bicycle\n  - Bus\n  - Motorbike\n  - Truck\n  - Tram\n  - Wheelchair\n"})}),"\n",(0,t.jsx)(e.h3,{id:"previous-class-mappings-for-reference",children:"Previous Class Mappings (for reference)"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fine-grained 8-Class"}),": Pedestrian, Car, Bicycle, Bus, Motorbike, Truck, Tram, Wheelchair"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Grouped 3-Class"}),": Pedestrian, Vehicle (Car, Bus, Truck, Tram), Micro-mobility (Bicycle, Motorbike, Wheelchair)"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"annotation-characteristics",children:"Annotation Characteristics"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Alignment"}),": Annotations synchronized with event timestamps"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Tracking"}),": Each object has a unique track_id across frames"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Confidence"}),": Detection confidence scores"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Density"}),": ~200-300 annotations per sequence"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"sample-sequence-analysis",children:"Sample Sequence Analysis"}),"\n",(0,t.jsxs)(e.p,{children:["Example statistics from ",(0,t.jsx)(e.code,{children:"train_night_0040"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Events"}),": 17,428,542 events over 5.72 seconds"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Annotations"}),": 212 annotations across 78 unique timestamps"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Classes"}),": Pedestrian (134), Car (78)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Bbox Sizes"}),": w=294.9\xb1148.9, h=161.7\xb140.7 pixels"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Event Rate"}),": ~3M events/second"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"data-loading-implementation",children:"Data Loading Implementation"}),"\n",(0,t.jsx)(e.h3,{id:"key-components",children:"Key Components"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"UltraLowMemoryLoader"}),": Main data loading class"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"EventProcessor"}),": Event-to-frame conversion utilities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"eTraMDataset"}),": PyTorch Dataset wrapper"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Streaming Support"}),": Real-time event processing"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"dynamic-batching-process",children:"Dynamic Batching Process"}),"\n",(0,t.jsx)(e.p,{children:"The eTraM data loader uses a sophisticated dynamic batching system:"}),"\n",(0,t.jsx)(e.h4,{id:"1-dynamic-sample-calculation",children:"1. Dynamic Sample Calculation"}),"\n",(0,t.jsx)(e.p,{children:"Instead of fixed samples per file, the system calculates the exact number of samples needed:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"train_day_0001.h5"}),": 1,500,000 events \u2192 150 samples (10K events each)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"train_night_0040.h5"}),": 17,428,542 events \u2192 1,743 samples (10K events each)"]}),"\n"]}),"\n",(0,t.jsx)(e.h4,{id:"2-overlapping-window-sampling",children:"2. Overlapping Window Sampling"}),"\n",(0,t.jsx)(e.p,{children:"Each sample uses overlapping windows to ensure complete coverage:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Use overlapping windows to ensure full coverage\noverlap = self.max_events_per_sample // 4  # 25% overlap\nstep_size = self.max_events_per_sample - overlap  # 7,500 events step\n\nstart_idx = sample_idx * step_size\nend_idx = min(start_idx + self.max_events_per_sample, total_events)\n"})}),"\n",(0,t.jsx)(e.h4,{id:"3-temporal-annotation-matching",children:"3. Temporal Annotation Matching"}),"\n",(0,t.jsx)(e.p,{children:"Annotations are temporally matched to the specific events being processed:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def _load_annotations_for_events(self, h5_file_path, events):\n    """Load annotations that match the specific events temporally"""\n    \n    # Extract timestamps from loaded events\n    event_timestamps = events[:, 2]  # Events are [x, y, t, p]\n    start_time = float(event_timestamps.min())\n    end_time = float(event_timestamps.max())\n    \n    # Add buffer for timing variations\n    time_buffer = (end_time - start_time) * 0.2  # 20% buffer\n    start_time -= time_buffer\n    end_time += time_buffer\n    \n    # Filter annotations by time window\n    all_annotations = self._load_all_annotations(h5_file_path)\n    time_mask = (all_annotations[\'t\'] >= start_time) & (all_annotations[\'t\'] <= end_time)\n    filtered_annotations = all_annotations[time_mask]\n    \n    return filtered_annotations\n'})}),"\n",(0,t.jsx)(e.h2,{id:"memory-requirements",children:"Memory Requirements"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Event Data"}),": ~200MB per sequence (HDF5 compressed)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Annotations"}),": ~50KB per sequence"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Processing"}),": ~500MB RAM for real-time processing"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"camera-specifications",children:"Camera Specifications"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Model"}),": Prophesee EVK4 HD"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor"}),": Sony IMX636 Event-Based Vision Sensor"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Resolution"}),": 1280\xd7720 pixels"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Range"}),": >86 dB"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Resolution"}),": >10,000 fps"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,s)=>{s.d(e,{R:()=>r,x:()=>l});var i=s(6540);const t={},a=i.createContext(t);function r(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);